{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune and Evaluate LoRA Model\n",
        "\n",
        "This notebook orchestrates the complete workflow:\n",
        "1. Hugging Face authentication\n",
        "2. Model training using config.yaml\n",
        "3. Model evaluation\n",
        "4. Upload trained model to Hugging Face Hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import wandb\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Import utility functions\n",
        "from utils.config_utils import load_config\n",
        "from utils.data_utils import load_and_prepare_dataset\n",
        "from utils.hf_utils import authenticate_huggingface\n",
        "from utils.model_utils import setup_model_and_tokenizer\n",
        "from train_qlora import train_model\n",
        "from evaludate_qlora import evaluate_peft_model\n",
        "from paths import OUTPUTS_DIR\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Authenticate with Hugging Face\n",
        "\n",
        "Authenticate to access gated models and upload your trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with Hugging Face\n",
        "# This will check for HUGGINGFACE_TOKEN or HF_TOKEN environment variables\n",
        "# Or prompt for interactive login\n",
        "authenticate_huggingface()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Configuration\n",
        "\n",
        "Load model and training configuration from config.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from config.yaml\n",
        "cfg = load_config()\n",
        "\n",
        "print(\"üìã Configuration loaded:\")\n",
        "print(f\"  Base Model: {cfg['base_model']}\")\n",
        "print(f\"  Dataset: {cfg['dataset']['name']}\")\n",
        "print(f\"  Learning Rate: {cfg['learning_rate']}\")\n",
        "print(f\"  Batch Size: {cfg['batch_size']}\")\n",
        "print(f\"  Epochs: {cfg['num_epochs']}\")\n",
        "print(f\"  LoRA r: {cfg['lora_r']}\")\n",
        "print(f\"  LoRA alpha: {cfg['lora_alpha']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Dataset\n",
        "\n",
        "Load and prepare the training and validation datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset splits\n",
        "train_data, val_data, test_data = load_and_prepare_dataset(cfg)\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded:\")\n",
        "print(f\"  Training samples: {len(train_data)}\")\n",
        "print(f\"  Validation samples: {len(val_data)}\")\n",
        "print(f\"  Test samples: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Setup Model and Tokenizer\n",
        "\n",
        "Initialize the base model with quantization and LoRA configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup model with 4-bit quantization and LoRA\n",
        "model, tokenizer = setup_model_and_tokenizer(\n",
        "    cfg, \n",
        "    use_4bit=True, \n",
        "    use_lora=True, \n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Initialize Weights & Biases (Optional)\n",
        "\n",
        "Initialize W&B for experiment tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize W&B for experiment tracking\n",
        "wandb.init(\n",
        "    project=cfg.get(\"wandb_project\", \"samsum\"),\n",
        "    name=cfg.get(\"wandb_run_name\", \"lora-finetuning-default-hps\"),\n",
        "    config={\n",
        "        \"model\": cfg[\"base_model\"],\n",
        "        \"learning_rate\": cfg.get(\"learning_rate\", 2e-4),\n",
        "        \"epochs\": cfg.get(\"num_epochs\", 1),\n",
        "        \"lora_r\": cfg.get(\"lora_r\", 8),\n",
        "        \"lora_alpha\": cfg.get(\"lora_alpha\", 16),\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"‚úÖ W&B initialized!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train the Model\n",
        "\n",
        "Fine-tune the model using LoRA on the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "train_model(\n",
        "    cfg,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_data,\n",
        "    val_data,\n",
        "    save_dir=cfg.get(\"save_dir\", None),\n",
        ")\n",
        "\n",
        "# Finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate the Trained Model\n",
        "\n",
        "Evaluate the fine-tuned model on the validation set using ROUGE metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "# The adapter_dir will default to OUTPUTS_DIR/lora_samsum/lora_adapters\n",
        "scores, preds = evaluate_peft_model(cfg)\n",
        "\n",
        "print(\"\\nüìä Final Evaluation Results:\")\n",
        "print(f\"  ROUGE-1: {scores['rouge1']:.2%}\")\n",
        "print(f\"  ROUGE-2: {scores['rouge2']:.2%}\")\n",
        "print(f\"  ROUGE-L: {scores['rougeL']:.2%}\")\n",
        "\n",
        "# Show a sample prediction\n",
        "print(\"\\nüìù Sample Prediction:\")\n",
        "print(f\"  Prediction: {preds[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Upload Model to Hugging Face Hub\n",
        "\n",
        "Upload the trained LoRA adapters to Hugging Face Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "# Set your Hugging Face username and model name\n",
        "# TODO: Update these with your Hugging Face username and desired model name\n",
        "HF_USERNAME = \"your-username\"  # Replace with your Hugging Face username\n",
        "MODEL_NAME = \"llama-3.2-1b-samsum-lora\"  # Replace with your desired model name\n",
        "HF_REPO_ID = f\"{HF_USERNAME}/{MODEL_NAME}\"\n",
        "\n",
        "print(f\"üì§ Preparing to upload model to: {HF_REPO_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the adapter directory path\n",
        "adapter_dir = os.path.join(OUTPUTS_DIR, \"lora_samsum\", \"lora_adapters\")\n",
        "\n",
        "if not os.path.exists(adapter_dir):\n",
        "    raise FileNotFoundError(f\"‚ùå Adapter directory not found: {adapter_dir}\")\n",
        "\n",
        "print(f\"üìÇ Adapter directory: {adapter_dir}\")\n",
        "\n",
        "# Create repository on Hugging Face Hub (if it doesn't exist)\n",
        "api = HfApi()\n",
        "try:\n",
        "    create_repo(\n",
        "        repo_id=HF_REPO_ID,\n",
        "        repo_type=\"model\",\n",
        "        private=False,  # Set to True if you want a private repo\n",
        "        exist_ok=True,\n",
        "    )\n",
        "    print(f\"‚úÖ Repository created/verified: {HF_REPO_ID}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Repository creation note: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the adapter files to Hugging Face Hub\n",
        "print(f\"üì§ Uploading adapter files to {HF_REPO_ID}...\")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=adapter_dir,\n",
        "    repo_id=HF_REPO_ID,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=f\"Upload LoRA adapters fine-tuned on SAMSum dataset\\n\\nROUGE-1: {scores['rouge1']:.2%}\\nROUGE-2: {scores['rouge2']:.2%}\\nROUGE-L: {scores['rougeL']:.2%}\",\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model successfully uploaded to: https://huggingface.co/{HF_REPO_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "‚úÖ Training and evaluation complete!\n",
        "\n",
        "- **Model**: Fine-tuned LoRA adapters saved locally\n",
        "- **Evaluation**: ROUGE scores computed and saved\n",
        "- **Hugging Face**: Model uploaded to Hub\n",
        "\n",
        "You can now use your fine-tuned model from Hugging Face Hub:\n",
        "```python\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(cfg[\"base_model\"])\n",
        "model = PeftModel.from_pretrained(base_model, f\"{HF_USERNAME}/{MODEL_NAME}\")\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
